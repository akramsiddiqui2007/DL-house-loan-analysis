# -*- coding: utf-8 -*-
"""house_loan_analysis_fixed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-pfqtuVFOaepsWrnen94uMbG6n2Hn55G

# House Loan Data Analysis with Deep Learning
This notebook analyzes loan data to predict defaults using a deep learning model. Missing data is handled before training.

## Step 1: Load the dataset
"""

import pandas as pd

loan_data = pd.read_csv("loan_data.csv", encoding="ISO-8859-1")
data_dict = pd.read_csv("Data_Dictionary.csv", encoding="ISO-8859-1")

loan_data.head()

"""## Step 2: Check for missing values (EDA)"""

from collections import Counter
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.impute import SimpleImputer


# a. Check size of the DataFrame
print("Shape of DataFrame:", loan_data.shape)

# b. Check columns and their data types
print("\nColumn data types:\n", loan_data.dtypes)

# c. Check number of missing values per column (as %)
missing_percent = loan_data.isnull().mean() * 100
print("\nMissing value percentage per column:\n", missing_percent)

# c.1 Drop columns with more than 40% missing
cols_to_drop = missing_percent[missing_percent > 40].index
print("\nDropping columns with >40% missing values:\n", list(cols_to_drop))
loan_data.drop(columns=cols_to_drop, inplace=True)

# c.2 Impute columns with <=40% missing
for col in loan_data.columns:
    if loan_data[col].isnull().sum() > 0:
        if loan_data[col].dtype in ['float64', 'int64']:
            # use mean or median based on distribution
            if loan_data[col].skew() > 1:
                imputer = SimpleImputer(strategy='median')
            else:
                imputer = SimpleImputer(strategy='mean')
            loan_data[col] = imputer.fit_transform(loan_data[[col]])
        else:
            # For flags or categoricals: fill with 0 or 'missing'
            loan_data[col] = loan_data[col].fillna('missing')

# d. Check and encode categorical columns
categorical_cols = loan_data.select_dtypes(include=['object']).columns
print("\nCategorical columns:\n", categorical_cols.tolist())

# Apply one-hot encoding
loan_data_encoded = pd.get_dummies(loan_data, columns=categorical_cols, drop_first=True)

# e. Correlation matrix (for numeric columns)
plt.figure(figsize=(12, 10))
sns.heatmap(loan_data_encoded.corr(), cmap='coolwarm', annot=False)
plt.title("Correlation Matrix")
plt.show()

loan_data.isnull().sum().sort_values(ascending=False)

## SMOTE to make the data balanced

from imblearn.over_sampling import SMOTE
from collections import Counter

# Cleaning up column names
loan_data_encoded.columns = loan_data_encoded.columns.str.strip()

# Confirming TARGET
print("Columns:", loan_data_encoded.columns.tolist())
assert 'TARGET' in loan_data_encoded.columns

# Get X and y
X = loan_data_encoded.drop('TARGET', axis=1)
X = X.select_dtypes(include=['number'])
y = loan_data_encoded['TARGET']

# Check for NaNs
assert X.isnull().sum().sum() == 0, "X contains NaNs"
assert y.isnull().sum() == 0, "y contains NaNs"

# Plot before
sns.countplot(x=y)
plt.title("Class Distribution Before SMOTE")
plt.show()

# Applying SMOTE
smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X, y)

# Plot after
sns.countplot(x=y_smote)
plt.title("Class Distribution After SMOTE")
plt.show()

# Confirm balance
print("After SMOTE:", Counter(y_smote))

"""## Step 3: Analyze Target Class Distribution"""

# Count and normalize the TARGET column
default_counts = df['TARGET'].value_counts(normalize=True) * 100

# Format and print
print("Loan Repayment Status:")
print(f"No Default (TARGET = 0): {default_counts[0]:.2f}%")
print(f"Default (TARGET = 1): {default_counts[1]:.2f}%")

"""## Step 4: Balance the dataset using upsampling"""

from sklearn.utils import resample

df_majority = loan_data[loan_data.TARGET == 0]
df_minority = loan_data[loan_data.TARGET == 1]

df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)
balanced_data = pd.concat([df_majority, df_minority_upsampled])
balanced_data['TARGET'].value_counts()

"""## Step 5: Plot balanced class distribution"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='TARGET', data=balanced_data)
plt.title("Balanced Class Distribution")
plt.show()

"""## Step 6: Encode categorical variables"""

categorical_cols = balanced_data.select_dtypes(include='object').columns
balanced_encoded = pd.get_dummies(balanced_data, columns=categorical_cols, drop_first=True)
balanced_encoded.head()

"""## Step 7: Split, Impute missing values, and Scale"""

from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

X = balanced_encoded.drop("TARGET", axis=1)
y = balanced_encoded["TARGET"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

imputer = SimpleImputer(strategy='mean')
training_columns = X_train.columns  # Save column names before transformation
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""## Step 8: Build and train a deep learning model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Defining the model
model = Sequential([
    Dense(64, input_dim=X_train.shape[1], activation='relu'), # Hidden layer 1
    Dense(32, activation='relu'),                             # Hidden layer 2
    Dense(1, activation='sigmoid')                            # Output layer (binary classification)
])

# input dimension
input_dim = X_train.shape[1]
print(f"input dimension: {input_dim:.4f}")


# Compiling model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Error formula = binary cross entropy

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# loss accuracy
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy:.4f}")

"""## Step 9: Evaluate model performance"""

from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report, ConfusionMatrixDisplay
import numpy as np
import matplotlib.pyplot as plt

# 1. Clean predictions
y_pred_prob = model.predict(X_test).flatten()
mask = ~np.isnan(y_pred_prob)

y_test_clean = y_test[mask]
y_pred_prob_clean = y_pred_prob[mask]
y_pred_clean = (y_pred_prob_clean > 0.5).astype(int)

# 2. Sensitivity and AUC
cm = confusion_matrix(y_test_clean, y_pred_clean)
tn, fp, fn, tp = cm.ravel()
sensitivity = tp / (tp + fn)
auc = roc_auc_score(y_test_clean, y_pred_prob_clean)

# 3. Classification report
print("Classification Report:\n")
print(classification_report(y_test_clean, y_pred_clean))

# 4. Confusion matrix plot
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.grid(False)
plt.show()

# 5. ROC Curve
fpr, tpr, thresholds = roc_curve(y_test_clean, y_pred_prob_clean)
roc_auc = roc_auc_score(y_test_clean, y_pred_prob_clean)

print("Sensitivity (Recall):", round(sensitivity, 4))
print("AUC-ROC:", round(roc_auc, 4))

plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='darkorange', label=f"ROC Curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate (Recall)")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

# Function to predict loan default for a new applicant
def predict_new_applicant(new_applicant_dict, training_columns):
    import pandas as pd
    import numpy as np

    # Step 1: Convert dictionary to DataFrame
    new_applicant_df = pd.DataFrame([new_applicant_dict])

    # Step 2: One-hot encode categorical variables
    new_applicant_encoded = pd.get_dummies(new_applicant_df)

    # Step 3: Align with training columns using reindex
    new_applicant_encoded = new_applicant_encoded.reindex(columns=training_columns, fill_value=0)

    # Step 4: Impute and scale
    new_applicant_imputed = imputer.transform(new_applicant_encoded)
    new_applicant_scaled = scaler.transform(new_applicant_imputed)

    # Step 5: Predict
    prob = model.predict(new_applicant_scaled)[0][0]
    prediction = int(prob > 0.5)

    print(f"Predicted Default Probability: {prob:.4f}")
    print(f" Prediction: {'Default' if prediction == 1 else 'No Default'}")

    return prob, prediction

import pandas as pd

# Load dataset
df = pd.read_csv("loan_data.csv", encoding="ISO-8859-1")

# Separate columns by type
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

# Build example input dictionary
sample_applicant = {}

# Add numeric fields using median
for col in numerical_cols:
    sample_applicant[col] = df[col].median()

# Add categorical fields using first non-null category
for col in categorical_cols:
    sample_applicant[col] = df[col].dropna().unique()[0] if df[col].notnull().sum() > 0 else "example"

# Show sample input dictionary data
sample_applicant

# predict loan default for this new applicant (sample_applicant)
predict_new_applicant(sample_applicant, training_columns=training_columns)

# Creating a modified safe version of sample_applicant
safe_applicant = sample_applicant.copy()

# Increase income significantly
safe_applicant['AMT_INCOME_TOTAL'] = 300000

# Reduce loan amount
safe_applicant['AMT_CREDIT'] = 100000

# Improve external risk scores
safe_applicant['EXT_SOURCE_1'] = 0.95
safe_applicant['EXT_SOURCE_2'] = 0.98
safe_applicant['EXT_SOURCE_3'] = 0.93

# Improve age (younger = higher number of negative days)
safe_applicant['DAYS_BIRTH'] = -12000  # ~32 years old

# Shorter unemployment (better job status)
safe_applicant['DAYS_EMPLOYED'] = -300  # recently employed

# Predict
predict_new_applicant(safe_applicant, training_columns=training_columns)

# Finding safe candidates
safe_candidates = non_defaulters[
    (non_defaulters['AMT_INCOME_TOTAL'] > 150000) &
    (non_defaulters['AMT_CREDIT'] < 200000) &
    (non_defaulters['EXT_SOURCE_2'] > 0.7) &
    (non_defaulters['DAYS_EMPLOYED'] > -1000)
]

print(f"Found {len(safe_candidates)} candidates.")

if not safe_candidates.empty:
    real_safe_applicant = safe_candidates.iloc[0].drop("TARGET").to_dict()
    predict_new_applicant(real_safe_applicant, training_columns=training_columns)
else:
    print("No suitable applicant found.")